### Day 1: By committing to this challenge I want to make coding a habit whic
Today I am starting with a mini-project which involves providing a Data solution to a fictional music streaming startup (Sparkify), which is looking for cost-effective ways to store their OLAP tables for their analytics team to work on.

Sparkify, has grown their user base, song database and want to move their data existing warehouse to a data lake. The objective of the project is to build a ETL pipeline that does the following tasks.
 
	1. Read song and user log data (files) from the existing staging area (Amazon S3)
	2. Process the data using Spark (local or EMR cluster)
	3. Store the data in staging tables on multi-node Datawarehouse (Amazon Redshift Cluster)
	4. Convert the data into dimension tables (Star schema)
	5. Store the data back to a Data lake (Amazon S3)
