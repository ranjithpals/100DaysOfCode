## Project 1:
Today I am starting with a mini-project which involves providing a Data solution to a fictional music streaming startup (Sparkify), which is looking for cost-effective ways to store their OLAP tables for their analytics team to work on.

Sparkify, has grown their user base, song database and want to move their data existing warehouse to a data lake. The objective of the project is to build a ETL pipeline that does the following tasks.
 
	1. Read song and user log data (files) from the existing staging area (Amazon S3)
	2. Process the data using Spark (local or EMR cluster)
	3. Store the data in staging tables on multi-node Datawarehouse (Amazon Redshift Cluster)
	4. Convert the data into dimension tables (Star schema)
	5. Store the data back to a Data lake (Amazon S3)
	
## Day 15:
1. Move the ETL script (ETL.py) and config file using FTP client from local PC to Spark Cluster's master node.
2. Use open-source terminal emulator to login to the master node using SSH.
3. Install the required python libraries (configparser, pandas) using linux commands.
4. Execute the ETL python script in the EMR cluster from CLI.
5. View the status of the Spark Jobs using the Spark History Server UI.

## Day 14:
- Validated the artist (table) attributes of a unique log data record.
- Validated the users (table) attributes of a unique log data record.
- Validated the song (table) attributes of a unique log data record.

## Day 13:
1. Created a ERD diagram for the Datalake.

## Day 12:
1. Tried to run the etl pipeline pyspark script (.py) file from Amazon Systems Manager which is used to submit hadoop/spark jobs
securely without using SSH on Amazon EMR cluster using the Amazon Console.

## Day 10:
1. Not able to make much progress I faced multiple issues when trying to read the complete songs dataset from S3 bucket,
   and save the songs dimension table in S3 bucket. Will continue to make progress tomorrow.

## Day 09:
Read the output files from the star schema.
Performed data validation.
- Validated if the total number of records match between the log data and fact table
- Validated the records for a given year and month (songplays fact table partition) to the log data.

## Day 08:
After facing issues in saving parquet files on to S3 bucket, today I was able to resolve it!! The solution was to create the S3 bucket in the same region as the Cluster.

Today I started using a very useful feature in GitHub to create projects and associate a Kanban board to manage and track tasks/stories to work on daily. I will be using it to complete the remaining tasks in my project.

Created a Amazon EMR cluster.
Created a S3 bucket in the same region as the EMR cluster.
Modified the Jupyter notebook to run the program on EMR cluster.
Validated the parquet files were created in S3 bucket with partitions.

## Day 7:
1. Convert the Jupyter notebook solution to modular solution by writing it to a python file (.py) file.

## Day 6:
1. Create Fact table in the star schema with the following dimension tables (songs, artists, users and time_table)
2. Save all the dimension and fact tables in Parquet format in the local disk (I am facing issues for the past few days to store the distributed files directly in S3)
3. Create partitions on artist and song attributes for the fact table: songplays.

## Day 5:
1. Create a UDF for extracting the week day from epoch timestamp, and create users table.

## Day 4:
1. Tranform the start time in epoch time format to obtain the following columns
	a. hour
	b. day
	c. week
	d. month
	e. year

## Day 3:
1. Filter the columns from the users log data dataframe needed to create the users tables.
2. Filter the timestamp field from the user log data.
	a. Created a UDF to extract the hour from the epoch timestamp.
3. Save the songs table in Parquet format in S3 bucket, facing issues doing so.

## Day 2:
1. Create Boto3 (Python API for Amazon) clients for EC2, S3, IAM, and RedShift.
2. Filter the columns from the songs data dataframe needed to create the tables.
	a. songs table.
	b. artists table.
3. Create a S3 Bucket for a given region (default east) to store the dimension tables.

## Day 1:
1. Created a Jupyter Notebook to check if the basic connections of the pipeline can be created.
2. Created a configuration file to capture the following credentials.
   a. User access key id, secret key (AWS)
   b. Redshift Cluster configuration and end points (AWS DWH)
   c. Database credentials (DWH DB)
   d. Raw data files in S3 bucket (Location)
3. Created a Spark session (Local cluster)
4. Read the configuration details from the Config file.
4. Read the song data, user log data as JSON files from S3 bucket into a Spark DataFrame.
5. View the data in the DataFrame.

