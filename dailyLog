## Project 1:
Today I am starting with a mini-project which involves providing a Data solution to a fictional music streaming startup (Sparkify), which is looking for cost-effective ways to store their OLAP tables for their analytics team to work on.

Sparkify, has grown their user base, song database and want to move their data existing warehouse to a data lake. The objective of the project is to build a ETL pipeline that does the following tasks.
 
	1. Read song and user log data (files) from the existing staging area (Amazon S3)
	2. Process the data using Spark (local or EMR cluster)
	3. Store the data in staging tables on multi-node Datawarehouse (Amazon Redshift Cluster)
	4. Convert the data into dimension tables (Star schema)
	5. Store the data back to a Data lake (Amazon S3)

## Day 6:
1. Create Fact table in the star schema with the following dimension tables (songs, artists, users and time_table)
2. Save all the dimension and fact tables in Parquet format in the local disk (I am facing issues for the past few days to store the distributed files directly in S3)
3. Create partitions on artist and song attributes for the fact table: songplays.

## Day 5:
1. Create a UDF for extracting the week day from epoch timestamp, and create users table.

## Day 4:
1. Tranform the start time in epoch time format to obtain the following columns
	a. hour
	b. day
	c. week
	d. month
	e. year

## Day 3:
1. Filter the columns from the users log data dataframe needed to create the users tables.
2. Filter the timestamp field from the user log data.
	a. Created a UDF to extract the hour from the epoch timestamp.
3. Save the songs table in Parquet format in S3 bucket, facing issues doing so.

## Day 2:
1. Create Boto3 (Python API for Amazon) clients for EC2, S3, IAM, and RedShift.
2. Filter the columns from the songs data dataframe needed to create the tables.
	a. songs table.
	b. artists table.
3. Create a S3 Bucket for a given region (default east) to store the dimension tables.

## Day 1:
1. Created a Jupyter Notebook to check if the basic connections of the pipeline can be created.
2. Created a configuration file to capture the following credentials.
   a. User access key id, secret key (AWS)
   b. Redshift Cluster configuration and end points (AWS DWH)
   c. Database credentials (DWH DB)
   d. Raw data files in S3 bucket (Location)
3. Created a Spark session (Local cluster)
4. Read the configuration details from the Config file.
4. Read the song data, user log data as JSON files from S3 bucket into a Spark DataFrame.
5. View the data in the DataFrame.

